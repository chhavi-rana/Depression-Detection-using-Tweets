{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8adaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from math import log\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy.random import RandomState\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea638aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(message, lower_case = True, stem = True, stop_words = True, gram = 1):\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f813a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    #length of unique features in corpus, total unique features in Corpus\n",
    "    vocab_len = 0\n",
    "    #Matrix: total tf-idf weight per term in a document within Corpus\n",
    "    sum_tf_idf_weights_all_terms = 0\n",
    "    #Total rows in Corpus\n",
    "    rows_topic = 0\n",
    "    #tf-idf weight per term in document\n",
    "    vocab_per_document = 0\n",
    "\n",
    "    def __init__(self, csv_file, topic):\n",
    "        \n",
    "        #Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english', analyzer='word', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n",
    "                                     ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "        #Learn vocabulary and idf, return term-document matrix.\n",
    "        term_document_matrix = vectorizer.fit_transform(self.doc_generator(csv_file, topic, textcol=1, skipheader=True))\n",
    "\n",
    "        #Number of unique features\n",
    "        self.vocab_len = len(vectorizer.get_feature_names())\n",
    "\n",
    "        col = [i for i in vectorizer.get_feature_names()]\n",
    "        vocab_per_document = pd.DataFrame(term_document_matrix.todense(), columns=col)\n",
    "        self.rows_topic = vocab_per_document.shape[0]\n",
    "\n",
    "        #total sum tf-idf per term in corpus\n",
    "        self.tf_idf_per_term = vocab_per_document.sum(axis=0, skipna=True)\n",
    "        sum_tf_idf_weights_all_terms_temp = 0\n",
    "        for i in self.tf_idf_per_term:\n",
    "            sum_tf_idf_weights_all_terms_temp += i\n",
    "\n",
    "        #total tf-idf weight for all terms.\n",
    "        self.sum_tf_idf_weights_all_terms = sum_tf_idf_weights_all_terms_temp\n",
    "\n",
    "\n",
    "    #Returns tf-idf weight per term in Corpus\n",
    "    def get_term_tf_idf(self,term):\n",
    "        try:\n",
    "            return self.tf_idf_per_term.loc[term]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "    #Document generator, reads CSV, skips header, reads a line in CSV, applies stemming & lamemmatixers\n",
    "    \n",
    "    def doc_generator(self,filepath, topic, textcol=0, skipheader=True):\n",
    "\n",
    "        porter = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        with open(filepath, encoding=\"utf8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            if skipheader:\n",
    "                next(reader, None)\n",
    "            if (topic == '-1'):\n",
    "                for row in reader:\n",
    "                   stem = self.stemSentence(porter, row[textcol])\n",
    "                   yield stem\n",
    "            else:\n",
    "                for row in reader:\n",
    "                    if (topic == row[2]):\n",
    "                        stem = self.stemSentence(porter, row[textcol])\n",
    "                        yield stem\n",
    "\n",
    "\n",
    "    def stemSentence(self,porter, sentence):\n",
    "        token_words = word_tokenize(sentence)\n",
    "\n",
    "        stem_sentence = []\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(porter.stem(word))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n",
    "\n",
    "    def lemmatizeSentence(self,lemmatizer, sentence):\n",
    "        token_words = word_tokenize(sentence)\n",
    "\n",
    "        stem_sentence = []\n",
    "        for word in token_words:\n",
    "            stem_sentence.append(lemmatizer.lemmatize(word))\n",
    "            stem_sentence.append(\" \")\n",
    "        return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0727bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier(object):\n",
    "    Corpus = None\n",
    "    Corpus_Non_Depressed = None\n",
    "    Corpus_Depressed = None\n",
    "    feature_names_size = 0\n",
    "    total_docs = 0\n",
    "    Non_Depressed_docs = 0\n",
    "    Depressed_docs = 0\n",
    "    p_non_depressed_topic = 0\n",
    "    p_depressed_topic = 0\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        print(\n",
    "            'Converting a collection of ENTIRE Corpus to a matrix of TF-IDF features using TfidfVectorizer with ngram_range(1,2)..')\n",
    "        #Build entire corpus that includes depressed and non-depressed topics.\n",
    "        self.Corpus = Corpus(csv_file, '-1')\n",
    "        #vocabulary lengh (feature names length) in corpus.\n",
    "        self.feature_names_size = self.Corpus.vocab_len\n",
    "        #total docs in Corups\n",
    "        self.total_docs = self.Corpus.rows_topic\n",
    "\n",
    "        print(\n",
    "            'Converting a NON-DEPRESSED Corpus to a matrix of TF-IDF features using TfidfVectorizer with ngram_range(1,2)..')\n",
    "        #non-depressed corpus\n",
    "        self.Corpus_Non_Depressed = Corpus(csv_file, '0')\n",
    "        #total non-depressed documents\n",
    "        self.Non_Depressed_docs = self.Corpus_Non_Depressed.rows_topic\n",
    "        # sum of all tf-idf term weights for non-depressed documents\n",
    "        self.non_depressed_sum_tf_idf_weights_all_terms = self.Corpus_Non_Depressed.sum_tf_idf_weights_all_terms\n",
    "\n",
    "        print(\n",
    "            'Converting a DEPRESSED Corpus to a matrix of TF-IDF features using TfidfVectorizer with ngram_range(1,2)..')\n",
    "        # depressed corpus\n",
    "        self.Corpus_Depressed = Corpus(csv_file, '1')\n",
    "        self.Depressed_docs = self.Corpus_Depressed.rows_topic\n",
    "        self.depressed_sum_tf_idf_weights_all_terms = self.Corpus_Depressed.sum_tf_idf_weights_all_terms\n",
    "\n",
    "        #probability of non-depressed documents\n",
    "        self.p_non_depressed_topic = log(self.Non_Depressed_docs / self.total_docs)\n",
    "        # probability of depressed documents\n",
    "        self.p_depressed_topic = log(self.Depressed_docs / self.total_docs)\n",
    "\n",
    "\n",
    "    def Naive_Bayes_Classify(self,tweet):\n",
    "\n",
    "        #Calculate probability of non-depressed sentiment for given tweet\n",
    "        probability_non_depressed = 0\n",
    "        for term in tweet:\n",
    "            # Apply Laplace smoothing\n",
    "            tf_idf_per_term = self.Corpus_Non_Depressed.get_term_tf_idf(term)\n",
    "            #Use log to preserve precision instead of multiplication\n",
    "            probability_non_depressed += log((tf_idf_per_term + 1) / (self.non_depressed_sum_tf_idf_weights_all_terms + self.feature_names_size))\n",
    "        probability_non_depressed += self.p_non_depressed_topic\n",
    "\n",
    "        # Calculate probability of depressed sentiment for given tweet\n",
    "        probability_depressed = 0\n",
    "        for term in tweet:\n",
    "            #Laplace smoothing\n",
    "            tf_idf_per_term = self.Corpus_Depressed.get_term_tf_idf(term)\n",
    "            probability_depressed += log((tf_idf_per_term + 1) / (self.depressed_sum_tf_idf_weights_all_terms + self.feature_names_size))\n",
    "        probability_depressed += self.p_depressed_topic\n",
    "\n",
    "        if (probability_non_depressed >= probability_depressed):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    #Run classification on test-data and let system predict sentiment and output it to a file.\n",
    "    def predict(self,testData):\n",
    "        result = []\n",
    "        for i, r in testData.iterrows():\n",
    "            processed_message = process_message(r['message'])\n",
    "            result.append(int(self.Naive_Bayes_Classify(processed_message)))\n",
    "\n",
    "        pd.options.mode.chained_assignment = None\n",
    "        testData['prediction'] = result\n",
    "        testData.to_csv('output_testdata_prediction.csv')\n",
    "        print('Applied classification on test-data, results are in output_testdata_prediction.txt file.. ')\n",
    "        print('Format of the output_testdata_prediction CSV file >> id (document ID),message (tweet),label (given sentiment), prediction')\n",
    "        return testData\n",
    "\n",
    "    \n",
    "    #Run the metrics on system prediction on test-data and calculate metrics on human vs system prediction.\n",
    "    def metrics(self,testData):\n",
    "        print('Calculating precision, re-call, accurancy, F-score on test-data prediction. This will compare human prediction and system prediction')\n",
    "        true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "        for index, row in testData.iterrows():\n",
    "            label = int(row['label'])\n",
    "            prediction = int(row['prediction'])\n",
    "\n",
    "            true_pos += int(label == 0 and prediction == 0)\n",
    "            true_neg += int(label == 1 and prediction == 1)\n",
    "            false_pos += int(label == 1 and prediction == 0)\n",
    "            false_neg += int(label == 0 and prediction == 1)\n",
    "\n",
    "            # true_pos += int(label == 1 and prediction == 1)\n",
    "            # true_neg += int(label == 0 and prediction == 0)\n",
    "            # false_pos += int(label == 0 and prediction == 1)\n",
    "            # false_neg += int(label == 1 and prediction == 0)\n",
    "\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        Fscore = 2 * precision * recall / (precision + recall)\n",
    "        accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F-score: \", Fscore)\n",
    "        print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3fa82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Train classifer with training data set------------------------------\n",
      "Reading Kaggle dataset twitter_sentiment data in file, sentiment_tweets3\n",
      "Format of the CSV file >> id (document ID),message (tweet),label (sentiment)\n",
      "Split data into 80% training data and 20% test data..\n",
      "Train the classifier using training data set...\n",
      "Converting a collection of ENTIRE Corpus to a matrix of TF-IDF features using TfidfVectorizer with ngram_range(1,2)..\n",
      "Converting a NON-DEPRESSED Corpus to a matrix of TF-IDF features using TfidfVectorizer with ngram_range(1,2)..\n",
      "Converting a DEPRESSED Corpus to a matrix of TF-IDF features using TfidfVectorizer with ngram_range(1,2)..\n",
      "----------------------Apply classifier on test data------------------------------\n",
      "Apply Multinomial Naïve Bayes algorithm classifier to generate sentiment i.e. reading tweet (message) from test data, predict if it's non-depressed(0) or depressed(1)\n",
      "Applied classification on test-data, results are in output_testdata_prediction.txt file.. \n",
      "Format of the output_testdata_prediction CSV file >> id (document ID),message (tweet),label (given sentiment), prediction\n",
      "----------------------------------------------------------\n",
      "Calculating precision, re-call, accurancy, F-score on test-data prediction. This will compare human prediction and system prediction\n",
      "Precision:  0.9584509883017346\n",
      "Recall:  0.9962264150943396\n",
      "F-score:  0.9769736842105263\n",
      "Accuracy:  0.9638009049773756\n",
      "----------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      " Interactive Tool for the model: \n",
      "\n",
      "\n",
      "\n",
      "Enter a tweet: (Type adieu to exit):i feel happy nowdays\n",
      "Sentiment:NON-DEPRESSED tweet\n",
      "Enter a tweet: (Type adieu to exit):i'm not depressed\n",
      "Sentiment:DEPRESSED tweet\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    csv_file_name = 'sentiment_tweets3'\n",
    "    try:\n",
    "        print('----------------------Train classifer with training data set------------------------------')\n",
    "      \n",
    "        print('Reading Kaggle dataset twitter_sentiment data in file, ' + csv_file_name)\n",
    "        print('Format of the CSV file >> id (document ID),message (tweet),label (sentiment)')\n",
    "        df = pd.read_csv(csv_file_name + '.csv')\n",
    "        rng = RandomState()\n",
    "\n",
    "        print('Split data into 80% training data and 20% test data..')\n",
    "        #Split data set into 80% trining data set and remaining 20% for test data set.\n",
    "        trainData = df.sample(frac=0.7, random_state=rng)\n",
    "        testData = df.loc[~df.index.isin(trainData.index)]\n",
    "\n",
    "        #create 2 seperate files one for training and other for test.\n",
    "        trainData.to_csv(csv_file_name + '_train.csv', index=False)\n",
    "        testData.to_csv(csv_file_name + '_test.csv', index=False)\n",
    "\n",
    "        print('Train the classifier using training data set...')\n",
    "        #Train the classifier using training data set.\n",
    "        tweetClassifier = TweetClassifier(csv_file_name + '_train.csv')\n",
    "\n",
    "        print('----------------------Apply classifier on test data------------------------------')\n",
    "        print('Apply Multinomial Naïve Bayes algorithm classifier to generate sentiment i.e. reading tweet (message) from test data, predict if it\\'s non-depressed(0) or depressed(1)')\n",
    "        #Predict sentiment using Multinomial Naïve Bayes algorithm classifier, generate sentiment i.e. reading tweet (message), predict if it's non-depressed(0) or depressed(1)\n",
    "        results = tweetClassifier.predict(testData)\n",
    "\n",
    "\n",
    "        #Calculate precision, recall, F-score and accuracy on prediction - This step will compare human prediction and system prediction and calculates precision, recall..\n",
    "        print('----------------------------------------------------------')\n",
    "        tweetClassifier.metrics(results)\n",
    "        print('----------------------------------------------------------')\n",
    "\n",
    "\n",
    "        print('\\n\\n\\n Interactive Tool for the model: \\n\\n\\n')\n",
    "        while True:\n",
    "            tweet = input(\"Enter a tweet: (Type adieu to exit):\")\n",
    "            if tweet == \"adieu\":\n",
    "                break\n",
    "            else:\n",
    "                processed_message = process_message(tweet)\n",
    "                if (int(tweetClassifier.Naive_Bayes_Classify(processed_message))):\n",
    "                    print('Sentiment:' + 'DEPRESSED tweet')\n",
    "                else:\n",
    "                    print('Sentiment:' + 'NON-DEPRESSED tweet')\n",
    "    except FileNotFoundError:\n",
    "        print('Reading dataset twitter_sentiment data in file, ' + csv_file_name + ',not found!')\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a4216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
